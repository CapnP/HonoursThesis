%%
%% Template chap2.tex
%%

\chapter{Implementing Fingerprint Indexing}
\label{cha:method}

Re-iterate goals

\section{Structure of \Beagle}
\label{sec:initial}

Making any extension to the \beagle\ project (or any sizeable project
for that matter) will obviously require a solid
understanding of the existing codebase. This section provides an overview
of any existing Scala classes and their structure which is relevant to the implementation of the Fingerprint Index.

\subsection{Syntax and Data Structures}

The first aspect of \beagle\ we must examine is its existing data structures;
since our term indexer must be able to understand the structure of \beagle's internal
logical objects.

Figure \ref{fig:expressions} shows how first-order logic terms are stored. Terms
are contained within an Eqn (for Equation) object, which may be ordered ($\to$)
or unordered ($=$). Equations are then directly passed to a Literal container
which stores whether the Equation is positive or negative (true or false). A list
of Literals is maintained for each Clause which are in turn stored in a ClauseSet.
These lists are to be interpreted in \emph{Conjunctive Normal Form} (See Section \ref{sec:fol})
and thus their ordering is not relevant; save for retrieving specific Clauses / Literals.

This structure is fairly typical and quite directly translates the definitions
from Section \ref{sec:fol}. The structure could potentially be shortened by
removing the Eqn object and having Literals directly contain the left and right Terms;
but this would be inconsistent with most first-order logic literature and could cause confusion.

\begin{figure}[H]
  \centering
  \includegraphics[clip,trim=2.5cm 5cm 13cm 2cm,width=0.75\textwidth]{resources/logicstructure}
  \caption
   {Class structure for internal representation of logical formulae.}
   \label{fig:expressions}
\end{figure}

The object of most concern to our Term Indexer is, naturally, the Term object.
We must be able to pull apart Terms in order to sample them at various positions
to build indexing fingerprints (see Section \ref{sec:fingerprint}). The Term
class itself is actually an \emph{abstract} class with two different primary cases:
\begin{itemize}
\item[] \textbf{FunTerm:} Used to express a function application. Consists of
a function operator and a (possibly empty) list of arguments. Each argument
is another Term object.
\item[] \textbf{Var:} Used to express variables. Variables have a \emph{name}
and a \emph{sort}, used to identify if the variable is in the foreground
or background (i.e. an \emph{abstraction} variable, see Section \ref{sec:hier})
\end{itemize}

For example, the first-order logic term $f(a, g(x))$ (where $x$ is a foreground
variable and other symbols are functions of the appropriate arity) would be expressed as:
\[\text{FunTerm}(f, [\text{FunTerm}(a, []), \text{FunTerm}(g, [\text{Var}(x, FG)])] )\]
Knowledge of this structure will be very useful when we move on to constructing
Term Fingerprints in Section \ref{sec:fpbuild}.

\subsection{Main Inference Procedure}
\label{sec:mloop}

Now that we have a solid understanding of \beagle's relevant data structures
we may move on to examining the main loop of the program. This loop repeatedly
attempts all the inference rules in the {\HSWAC} (see Section \ref{sec:calc})
to generate new information. It also includes some optional rules and optimisations
which are not strictly part of the calculus, but drastically increase performance
in special cases. This includes:
\begin{itemize}
\item \textbf{Simplification:} Removes redundant variables and pre-processes some simple clauses.
Covered in detail in Section \ref{sec:simp}.
\item \textbf{The Split rule:} In some cases the Literals of a Clause may be partitioned
in two; with each partition consistent with our current knowledge. The Split rule allows
us to create a whole new instance of \beagle's main loop, to be run in parallel,
so that we may consider both options. These `\emph{branches}' may be closed if they return
unsatisfiable.
\item \textbf{The Instantiate rule:} Applies to Clauses with background variables in `\emph{finite domains}'.
If there are finitely many terms which a variable may represent it is sometimes useful
to remove that variable and replace it with one Clause per possible instantiation.
\end{itemize}

In Listing \ref{lst:main} we present a simplified pseudocode version of the main inference loop.
\verb!input ClauseSet! here represents our database of knowledge along with
the \emph{negation} of what we are trying to prove.
\begin{listing}[H]
\begin{lstlisting}
new := input ClauseSet
old := empty ClauseSet
While new is not empty
   select := Pop a clause from new
   simpl  := |\textbf{Simplify(select,new,old)}|
   If simpl is a tautology:
       Continue
   If simpl is the empty clause:
       return UNSAT
   If one of the Define, Split or Instantiate rules apply to simpl:
       new := new U ApplicableRule(simpl)
       Continue
   old := old U simpl
   Attempt all inference rules:
       new := new U EqualityResolution(simpl)
       new := new U EqualityFactoring(simpl)
       new := new U |\textbf{Superposition(simpl,old)}|
end While
\end{lstlisting}
\caption{Pseudocode for \beagle's main inference procedure.}
\label{lst:main}
\end{listing}

Notice the two bolded subroutines in the main procedure. All other
routines in the main loop require only the input of \verb!simpl!, but these two
also require \verb!old! and/or \verb!new!. This means that their runtime is dependant
on the size of the current ClauseSets; and considering that both sets grow overtime
these two functions are likely to dominate \beagle's runtime.

So \emph{simplification} and \emph{superposition} are the two main areas
we should target for improvement with indexing. This is consistent
with our earlier analysis of the abstract calculus (see Section \ref{sec:shortcomings})
where superposition was identified as the most costly inference rule.

\section{Building the Fingerprint Indexer}
\label{sec:initial}

The first step in adding fingerprint indexing to \beagle\ is creating the indexer
itself; an object which will manage the index and provide functions for adding
to it and retrieving from it.

This Section details the creation of the FingerprintIndex Scala class. It contains
all the data types and functions we will need for indexing; including building
and comparing term fingerprints, addition and retrieval from a complex index structure
and any auxiliary functions to assist with these computations.

\subsection{Objects and Data Types}
\label{sec:datatypes}

Here we define in detail any data structures which will be a part of our index.
These data structures must be capable of expressing any concepts from
the abstract definition of fingerprint indexing, outlined in Section \ref{sec:fingerprint}
and the original paper\cite{shulz12}.

\subsubsection{Positions}

We implement positions in the simple na\"{\i}ve manner, as a list of
Integers (where Nil is used to index the top-level term). This
directly reflects our position notation given in Section \ref{sec:terminology}.

\subsubsection{Fingerprint Features}

Fingerprint features are the four possible symbols we get when sampling a term
at an arbitrary position. The meaning of these features is given in Section \ref{sec:fingerprints},
so here we provide only the Scala definition. We essentially only require an
enumerated type for Fingerprint Features, except for the fact that we must be able
to specify a function symbol. Thus we implement this type as four separate classes
implementing an abstract \verb!FPFeature! class; with one of these classes taking
a function name as a parameter.

\begin{listing}[H]
\begin{scalacode}
/** Pseudo enumerated type for fingerprint features */
sealed abstract class FPFeature
case object FPA extends FPFeature 
case object FPB extends FPFeature
case object FPN extends FPFeature
case class  FPF(val f : String) extends FPFeature
\end{scalacode}
\caption{Data type for the 4 Fingerprint Features \protect\cite[p5]{shulz12}}
\label{lst:featuredata}
\end{listing}

\subsubsection{Term Fingerprints}

With Positions and Fingerprint Features defined it is now very simple
to define the Fingerprint for a Term. We take this as simply a list
of Fingerpint Features, to be acquired by sampling at various positions. 

\subsubsection{Fingerprint Index}

The final data structure we require is the actual Index itself, a structure
which stores all the indexed terms and their Fingerprints so that they may be later
retrieved.

The na\"{\i}ve method for implementing the Index would be to simply use a HashMap
from Fingerprints to their corresponding Term. This method would however cause
several problems which would make the correct and efficient retrieval of terms impossible.
A term's Fingerprint does not only match itself but also matches any compatible
Fingerprints with respect to some comparison table (see Section \ref{sec:fingerprints}).
So our Index object must store Terms in a way that allows compatible sets to be
collected together. In Listing \ref{lst:indexdata} we present an algebraic data
type for an Index, structured as a tree of HashMaps. Each Index is either a
collection of Terms (a Leaf) or a mapping from FPFeatures to more Index objects (a Node).

\begin{listing}[H]
\begin{scalacode}
/** Algebraic Data type for our index. Either we are at a leaf
  * (set of terms) or must continue traversal via the map. */ 
sealed abstract class Index
case class Leaf(set: Set[Term])                 extends Index
case class Node(map: HashMap[FPFeature, Index]) extends Index
\end{scalacode}
\caption{Data type for the actual term index. \protect\cite[p7]{shulz12}}
\label{lst:indexdata}
\end{listing}

Note that this Index object does not necessarily take up a significant portion of memory.
All Terms are already stored within the ClauseSet object (See Figure \ref{fig:expressions});
so the Index itself will generally only add a fairly lightweight structure of pointers.


\subsection{Building Term Fingerprints}
\label{sec:fpbuild}
With our required data structures in place we may now begin implementing our
Fingerprint Index proper. There are two main components in this implementation:
adding to, and removing from the index. A logical first choice is addition; the
first step of which is creating functions to sample terms at positions in order to
create term fingerprints.

Listing \ref{lst:posextract} provides a function to extract a single Fingerprint Feature from
a Term at the given position.
\begin{listing}[H]
\begin{scalacode}
/** Extract the FPFeature at Position pos of the given Term object. */
def extractFeature(term: Term, pos: Position) : FPFeature = pos match {
  // Reached end of position, check symbol
  case Nil     => term match {
    case t:FunTerm => FPF(t.op) // Found function symbol, return it
    case t:Var     => FPA       // Found variable, return A
  }
  case p :: ps => term match {
    case t:FunTerm => try   {extractFeature(t.args(p), ps)}
                      //Non-existent position, return N
                      catch {case e:IndexOutOfBoundsException => FPN}
    // Found variable BEFORE end of position, return B
    case t:Var     => FPB 
  }
}
\end{scalacode}
\caption{Scala code to extract fingerprint features for matching.}
\label{lst:posextract}
\end{listing}
This code is intended to be a fairly direct implementation of the four
fingerprint features described in Section \ref{sec:fingerprints}
(and \cite{shulz12}). We simply traverse through the Term until
we reach the desired position or we find a variable.

Generating the actual Fingerprint for a Term is now a straightforward process
of repeating this function for each desired Position.

\subsection{Adding Terms to the Index}
Now that we can generate Term Fingerprints we must use them to store Term objects
at the correct position of our Index data structure (see Section \ref{sec:datatypes}).
This is done by following the Index tree mappings for each Fingerprint Feature in
the Fingerprint. Traversing the tree is relatively complex; as at each
level we may need to create nodes in order to continue traversal. Listing \ref{lst:addterm}
presents code for simultaneously traversing the tree while creating Nodes and Leaves.
\begin{listing}[H]
\begin{scalacode}
/** Add a Term into the given Index. Traverses Index tree
  * (adding nodes where needed) and adds Term t to a Leaf set. */
private def add (t:Term, fp:Fingerprint, index: Index):Index =
(fp, index)  match {
  //Reached a leaf at the end of the Fingerprint. Add to set.
  case (Nil,   Leaf(set)) => Leaf(t::set)
  //Still traversing tree. Add new Node or Leaf if necessary
  case (f::fs, Node(map)) => (fs, map.get(f)) match {
    //Mapping exists. Traverse through it.
    case (_,   Some(index)) => {map += (f -> add(t, fs, index))
                                Node(map)}
    //At end of Fingerprint. Create Leaf and add to it
    case (Nil, None) => {map += (f -> Leaf(List(t)))
                         Node(map)}
    //Fingerprint not over. Create Node and continue traversing
    case (_,   None) => {val newIndex:Index = new Node()
                         map += (f -> newIndex)
                         add(t, fs, newIndex)
                         Node(map)}
  }
  case (_, Node(_)) => throw new IllegalArgumentException
                ("Fingerprint is over but we are not at a leaf")
  case (_, Leaf(_)) => throw new IllegalArgumentException
                ("Reached a leaf but Fingerprint is not over")
}
\end{scalacode}
\caption{Code to add a Term to the correct Leaf node of the Index data
structure defined in Section \ref{sec:datatypes}.}
\label{lst:addterm}
\end{listing}
\verb!add! is a recursive function which moves down the Index one step for
each time it is called. Notice that the function takes a Fingerprint as an argument. This argument
should initially be the Term's generated Fingerprint (relative to the Index's list of Positions);
but with each recursive call of \verb!add! we strip of one Fingerprint Feature and follow
it's mapping in the Index. Throughout this process we create new Index Nodes as required;
and at the end of the Fingerprint we create or add to a Leaf.
In accordance with programming best practices the final two cases in the Listing
throw meaningful error messages in the case of unexpected input.

We may now index an arbitrary Term object but it is desirable to be able to index
whole Clauses with a single function call. This is done with a straightforward lifting
over the expression syntax tree (Figure \ref{fig:expressions}) : Clauses index all of their Literals, Literals index
their Eqn and Eqns index each of their two Terms.

\subsection{Retrieving Compatible terms}
\label{sec:retrieve}

Our Index framework is now capable of creating Fingerprints and storing Terms in
its pointer structure. The next task in building our index is allowing retrieval
of Terms which are compatible with a query Term; relative to some comparison
table.

We will now provide an implementation of the Fingerprint comparison table for unification (Section \ref{sec:fingerprints}).
To compare two Fingerprints with each other we look at them side-by-side and return
\emph{true} or \emph{false} depending on how they match up in the table.
The na\"{\i}ve method for performing this check is to simply check for each possible
entry in the table manually. However, by examining the table more closely we observe
that it can be covered with only four cases:
\begin{enumerate}
\item True if the two Features are equal.
\item True if one of the Features is \textbf{B}.
\item True if one of the Features is \textbf{A}; but the other is not \textbf{N}.
\item False otherwise 
\end{enumerate}
We may implement these four cases in Scala by using the \verb!match! construct
to compare \verb!Set! objects.
\begin{listing}[H]
\begin{scalacode}
 /** Check two Fingerprint features for compatibility based
   * on the unification table (See page 6 of [Shulz 2012]). */
  def compareFeaturesForUnification
         (a:FPFeature, b:FPFeature) : Boolean =
  (a == b) || 
  (Set(a,b) match {
    case x if (x contains FPB) => true
    case x if (x contains FPA) => !(x contains FPN)
    case _ => false})
\end{scalacode}
\caption{Scala implementation of the Fingerprint unification table. \protect\cite[p6]{shulz12}}
\label{lst:unitable}
\end{listing}

To check whether or not two Fingerprints match is now a simple matter of iterating
through the list and checking that each position is a match according to our unification
table check. This side-by-side comparison could potentially be improved by employing
some variety of hashing function; however this sort of improvement does not
apply to our uses for term Fingerprints. Rather than comparing two Fingerprints
side-by-side we are only ever interested in retrieving \emph{all} compatible
terms from our Fingerprint Index (see Section \ref{sec:fpindex}
and Section \ref{sec:datatypes}).

So, rather than individually comparing the Fingerprints of each indexed Term,
we must build a function which traverses the Fingerprint Index tree structure
and collects all compatible terms.

\begin{listing}[H]
\begin{scalacode}
def retrieveCompatible (fp: Fingerprint, index: Index) : TermSet =
(fp, index) match {
//Collect all compatible (Feature,Index) pairs and continue traversal
    case (f::fs, Node(map)) => 
        {for ((k,v) <- map if compare(f, k))
            yield retrieveCompatible(fs, v)}
//Collapse all retrieved sets together with the union operator (:::)
        .foldLeft(Nil:TermSet) ((a,b) => a ::: b)
//Once we reach a Leaf we simply return the compatible set 
    case (Nil, Leaf(set)) => set
    case (_, Node(_)) => throw new IllegalArgumentException
         ("Fingerprint is over but we are not at a leaf")
    case (_, Leaf(_)) => throw new IllegalArgumentException
         ("Reached a leaf but Fingerprint is not over")
}
\end{scalacode}
\caption{Scala code to collect compatible terms from the index.}
\label{lst:retrieve}
\end{listing}

We present \verb!retrieveCompatible! (Listing \ref{lst:retrieve}); which takes
a Term, a Fingerprint and an Index and returns all Terms in the Index which
are compatible with respect to the \verb!compare! function.
\verb!compare! here is a function implementing a Fingerprint Feature comparison
table; such as \verb!compareFeaturesForUnification! from Listing \ref{lst:unitable}.
It can be passed to the Fingerprint Index as part of its configuration
object (see Section \ref{sec:config}).

\verb!retrieveCompatible! works similarly to \verb!add! from Listing \ref{lst:addterm};
recursively stripping off a Fingerprint Feature and traversing the Index tree.
The difference here is that we must `branch off', making a recursive call for
each feature which is compatible (according to \verb!compare!). The \verb!for-yield!
loop takes care of this, returning a list of TermSets to be collected together
\verb!foldLeft! and the union operation. As in Listing \ref{lst:addterm}, we cover
unexpected input cases with meaningful error messages.


\subsection{Matching with Subterms}

At this stage we have a complete implementation of abstract fingerprint indexing;
as described in Shulz's paper \cite{shulz12}. However, this index is not
quite at the point where it is usable in \beagle\ 
Recall the main superposition rules for \beagle's resolution calculus (Section \ref{sec:calc} and \cite{baum13}).
Notice in particular the condition that $s$ may match against a \emph{subterm}
of $l$ (this condition also exists in the original superposition calculus, Section \ref{sec:supcalc}).
This does not match our current implementation which is only capable of indexing
whole, top-level terms.

Thus our Fingerprint Index must be able to index and collect all possible matches against \emph{subterms}.
To do this we will extend \beagle's Term object with a function to generate all
subterms; along with the position they were extracted from. For variables and constants
this is trivial; we just return the symbol and Nil for the subterm position. For
functional terms however we must collect a list of each argument along with
all subterms of those arguments. Listing \ref{lst:subterms} performs this
operation by recursively finding the subterms of each argument and collecting
them together. 

\begin{listing}[H]
\begin{scalacode}
/** Retrieve all subterms along with their position */
def subtermsWithPos : List[(Term, List[Int])] = 
  (thisterm, Nil) :: (for 
    ((arg,    argpos)    <- args zip args.indices;
     (subarg, subargpos) <- arg.subtermsWithPos)
      yield  (subarg, argpos::subargpos))
\end{scalacode}
\caption{Recursively grab all subterms from a complex term.}
\label{lst:subterms}
\end{listing}
With this extension in place we can index subterms by extending our addition function
(Listing \ref{lst:addterm}) to also add all subterms.
So our indexer will now be capable of comprehensively indexing all subterms as required.
Our Index however is still unfit for live use in optimising \beagle's inference rules.
This is due to a subtle issue which prevents correct retrieval of all
the information needed by the inference calculus.

\subsection{Current Problems and Term Traces}
\label{sec:problems}
At this stage we have a Fingerprint Index which is capable of comprehensively indexing a Clause
all the way down to individual subterms. However, as previously mentioned,
there is a subtle issue remaining. We shall refer to this issue as \emph{Term Alienation}.
The issue was caught during the process of \emph{Unit Testing} (see Section {sec:unittest}) and relates
to retrieving the Clause structure associated with any Terms retrieved as compatible.

\subsubsection{Term Alienation}
In listing \ref{lst:indexdata} we presented the actual data structure for storing
indexed Terms. It presented the leaves of this index as simply a set of Term objects.
At this stage we notice that this representation is actually insufficient. \Beagle's
inference rules require knowledge of the Clauses which we are operating on, so
storing only the bottom level Term does not give us enough information to perform any
inferences. This is especially true considering that the stored Term may even
be a subterm of what we are actually wish to use for inference.

A first attempt to solve this issue involved giving every object in the expression
tree (Figure \ref{fig:expressions}) a pointer to its `\emph{parent}' expression.
Ensuring that these pointers were correct at all times however proved to be
difficult. Furthermore the pointers were difficult to use in practice; since there
are several steps involved in going from a subterm all the way up to a Clause.

\subsubsection{Term Tracing}

Term Alienation can be better solved by introducing \emph{Term Traces}. A Term Trace
is an object which will be stored alongside any Term in our Index; containing any
required information regarding where the Term originally came from. This includes
pointers to each object higher up in the expression syntax tree (the associated Eqn, Literal
and Clause) and the (possibly nil) subterm Position. Term Traces allow us to quickly
and easily retrieve any information required for inferences.
 
It is worth noting at this point that by indexing subterms and adding Term Traces we have
increased the size and complexity of our Index data structure considerably.
This is negligible however since we only introduce pointers rather than whole
copies of expressions. Even in the case of copying data this would be of little
concern since memory is cheap; and we are generally only concerned with speed.

\subsection{Unit Testing with ScalaTest}
\label{sec:unittest}

As with any component of a large software project; it is vital to ensure that the
Fingerprint Index functions on its own. Otherwise if there are issues when adding
the indexer to functional use there will be no way of knowing what component is
causing the problem.


\section{Adding Indexing to \Beagle}
Our Fingerprint Index class is now fully capable of indexing terms for inference
with the \HSWAC. Our task now is to add the indexer itself into \beagle's inference
loop and make use of it wherever appropriate.

Recall our analysis of \beagle's main loop from Section \ref{sec:mloop}; where we identified
the two sections most appropriate for being augmented with term indexing. We will
start by adding indexing to the \emph{superposition} inference rules; which is
the primary way \beagle\ and the \HSWAC\ creates new information.

\subsection{Attaching a Fingerprint Index}

Actually making use of our Fingerprint Index class will require significant modification
to \beagle's structure and proving sequence. In particular we will need
to add an Index object and replace any occurrences of searching for unification matches
to include indexing.

Originally indexing was included by adding a single Fingerprint Index to the main
class of \beagle. This index was initialised with all Clauses in the input knowledge set,
and was given new Clauses whenever they were created. This setup caused several
problems:
\begin{itemize}
\item \textbf{Redundant Clauses:} Some of \Beagle's operations (in particular Simplification, see Section \ref{sec:simprules})
can cause Clauses to become \emph{redundant}, and no longer required for inference.
These Clauses would remain in the Fingerprint Index; causing clutter and unnecessary
computation.
\item \textbf{Difficult to Split:} The Split rule (see Section \ref{sec:mloop})
could no longer be used since the Fingerprint Index could not easily be reproduced or
duplicated.
\item \textbf{No `Age' Differentiation:} Recall that \beagle\ maintains
two collections of Clauses, \verb!old! and \verb!new! (see Section \ref{sec:mloop}).
With only one Index we currently have no way of identifying which ClauseSet an indexed
Term has come from. As a result, superposition becomes unnecessarily cluttered;
as it only needs to be run against Clauses from \verb!old!.
\end{itemize}
These issues could potentially have been resolved with more careful management of
the Terms in our Index, but a more elegant solution exists.

Rather than attach a single Fingerprint Index to the entire \beagle\ inference
process, we instead add an Index to the ClauseSet class. 
Clauses from \verb!new! are removed one by one and possible made redundant before
moving to \verb!old!. \verb!old! itself however is more static; it only ever
has clauses \emph{added} to it. By only indexing the \verb!old! set we ensure
no redundant clauses appear in inferences; and it becomes easy to only use
the \verb!old! index for superposition.

Some thought is still required on how to fix the functionality of the Split rule.
Split must be able to copy \beagle's current state to create a new parallel instance;
so it must be able to copy a Fingerprint Index. When Split is activated it calls
the \verb!clone! method of the two main ClauseSets. We will have this method also copy
the associated Index. We can copy an Index either by re-adding all terms to a 
new Fingerprint Index or by creating a deep copy of the Index pointer structure.

\subsection{Indexing Superposition}
Now that our ClauseSets are actively being Indexed we must start to make
use of these Indices for superposition. As stated above
superposition will only require the use of the \verb!old! Clause Index; but there
is still significant modification required to use Index retrieved terms.

Recall the two superposition inference rules in the \HSWAC\ (taken unmodified
from \cite{baum13}):
\begin{align*}
\textbf{Positive Superposition} &&& \frac{l \approx r \lor C\quad \quad s[u] \approx t \lor D}{\text{abstr}((s[r] \approx t \lor C \lor D)\sigma)} 
\intertext{\tcent{Where
(i) $\sigma = $ simple mgu $(l,u)$,
(ii) $u$ is not a variable,
(iii) $r\sigma \not\succeq l\sigma$,
(iv) $t\sigma \not\succeq s\sigma$,\\
(v) $l$ and $u$ are not pure background terms,
(vi) $(l \approx r)\sigma$ is strictly maximal in $(l \approx r \lor C)\sigma$, and
(vii) $(s \approx t)\sigma$ is strictly maximal in $(s \approx t \lor D)\sigma$ }}
\textbf{Negative Superposition} &&& \frac{l \approx r \lor C\quad \quad s[u] \not\approx t \lor D}{\text{abstr}((s[r] \not\approx t \lor C \lor D)\sigma)}
\intertext{\tcent{Where 
(i) $\sigma = $ simple mgu $(l,u)$,
(ii) $u$ is not a variable,
(iii) $r\sigma \not\succeq l\sigma$,
(iv) $t\sigma \not\succeq s\sigma$,\\
(v) $l$ and $u$ are not pure background terms,
(vi) $(l \approx r)\sigma$ is strictly maximal in $(l \approx r \lor C)\sigma$, and
(vii) $(s \not\approx t)\sigma$ is strictly maximal in $(s \not\approx t \lor D)\sigma$ }}
\end{align*}

When \beagle\ runs these two rules it checks all Clauses in \verb!old! against
a single query Clause selected from \verb!new!. The query clause is tested
for being both the \emph{from} clause ($l \approx r \lor C$) or the \emph{into}
clause ($s[u] \approx t \lor D$). Note that \beagle\ does not split the superposition rules into
two distinct cases; but rather generates all possible negative and positive inferences simultaneously.

Our Fingerprint Index is built to locate Terms likely to \emph{unify}; so
it is condition (i) that our Indexer is most . Condition (i) states that
$l$ and $u$ must be unifiable by some simple most general unifier $\sigma$ (refer
to Sections \ref{sec:terminology} and \ref{sec:calc} for detailed definitions of
these terms). This condition implies that there are actually two distinct
cases we must cover for indexing; one where $l$ is the query term and one where $u$ is.
These cases correspond to the \emph{from} and \emph{into} cases mentioned
above.

\subsubsection{From Case}
In this case we have a query Clause $l \approx r \lor C$ and wish to find all
subterms $u$ which are likely to unify with the top level term $l$. Note that we must actually
attempt this for all possible $l$s in the Clause, so we must first loop over
each \emph{eligible} Literal. The eligible Literals are those which are positive and capable
of fulfilling condition (vi) in the rules above. If an eligible Literal is unordered we
must also try either side as $l$.

Once we have found each usable $l$ we simply retrieve all terms compatible for unification
(see Section \ref{sec:retrieve}) and use \beagle's existing superposition code
to confirm unification and check the other inference rules conditions.

\subsubsection{Into Case}
In this case we have a query Clause $s[u] \approx t \lor D$ and wish to find all
top level terms $l$ which are likely to unify with with a subterm $u$. As in the
from case we must find all possible terms which we can use for $s$; satisfying
maximality and ordering conditions. After selecting a term for $s$ we must 
also then loop over all its subterms, as any of these are a potential choice
for $u$.

Once we have a subterm for $u$ we retrieve compatible Terms from the index.
Unlike the from case we are not done here, $l$ may only be a top level term
so we must actually discard any subterms. This may sound expensive in terms
of wasted computation; but filtering in this fashion is cheap, and avoiding this
problem would require an entirely separate Index which indexes only top-level terms.
 
With the potential compatible values for $l$ now retrieved \beagle's existing code
can be used to complete the inference. Our two directional cases are now
covered and superposition is now using our Index to its full potential.

\section{Extending the Indexer}
\label{sec:simp}

\Beagle\ is now fully capable of indexing its terms for superposition. At this stage
we could look into how the Fingerprint Index could be improved; but it is likely
to be far more effective to re-examine where \beagle\ now spends most of its runtime
and investigate other inference procedures for which our Indexer could be applied to. 

We again refer to the analysis of \beagle's inference procedure from Section \ref{sec:mloop}.
In this Section we noted that there are only two subroutines of \beagle\ that require
searching through the ClauseSets; one being superposition and the other being simplification.
So simplification is the only other area where our Indexer may be applied to
any significant effect, and doing so is likely to provide a significant performance
increase.

To confirm how effective Indexing simplification could be we may refer to the results when
instrumenting \beagle\ in VisualVM. The results in Section \ref{section:instr} and \ref{section:instr2}
indicate (as expected) that simplification often takes up a significant portion
of \beagle's runtime.

We will now attempt to apply Fingerprint Indexing to simplification; beginning
by investigating the current implementation of \beagle's simplification rules. 

\subsection{Matching and Simplification in \Beagle}
\label{sec:simprules}
\Beagle\ has several simplification rules to aid the logical inference process.
These rules are not technically part of the actual rule based calculus
(hence they are not mentioned in section \ref{sec:calc}) but rather
implement some special cases of those rules. Providing separate implementations
of these cases can provide a significant speed-up in problems where they occur
frequently.

\subsubsection{Negative Unit Simplification}

\[ \textbf{Negative Unit Simplification} \quad\quad \frac{s\not\approx t \quad \quad s \approx t  \lor C}{C} \]

Note that we do not have a concept of \emph{Positive} Unit Simplification 
since it would be covered as a special case of the Demodulation rule.

\subsubsection{Demodulation}
The demodulation rule was first proposed for use in the superposition calculus by \citeN{demod}.

\begin{align*}
\textbf{Demodulation} &&& \frac{l \approx r \quad \quad s[u] \approx t  \lor D}{ (s[r] \approx t \lor D)\sigma}
\intertext{\tcent{Where
$\sigma = $ simple mgu $(l,u)$.\\
The clause $s[u] \approx t  \lor D$ may be removed.}}
\textbf{Negative Demodulation} &&& \frac{l \approx r \quad \quad s[u] \not\approx t  \lor D}{ (s[r] \not\approx t \lor D)\sigma}
\intertext{\tcent{Where
$\sigma = $ simple mgu $(l,u)$.\\
The clause $s[u] \not\approx t  \lor D$ may be removed.}}
\end{align*}
Demodulation allows us to more quickly remove variables and terms which are \emph{redundant}.

\subsection{Problems with Indexing Simplification}
\label{sec:simpprob}
Re-using current index produced little improvement. Cost of indexing
subterms, matching against equations with $\$equal$

\subsection{Generalising our Fingerprint Index}
\label{sec:config}

There is a simple solution to overcome the problems listed above. Creating multiple
indices. No longer restricted to the conditions of the superposition index.

This use of multiple indices obviously introduces a memory overhead. We
shall argue however that this overhead is negligible for the following reasons.

Here we introduce an options object to pass to our Fingerprint Index class.
This object will allow us to create multiple term indices that behave in different
ways.

\begin{listing}[H]
\begin{scalacode}
/** Configuration object for a Fingerprint Index */
class IndexConfig(
  val positionsToSample : PositionList,
  val indexSubterms     : Boolean,
  val indexPureBG       : Boolean,
  val eqnToTerm         : Boolean,
  val comparator        : (FPFeature, FPFeature) => Boolean)
\end{scalacode}
\caption{Class to pass settings to an arbitrary Fingerprint Index. Note that
this class does not require an implementation.}
\label{lst:config}
\end{listing}

\begin{itemize}
\item \textbf{positionsToSample}: A list of positions indicating what should be sampled
to create term fingerprints.
\item \textbf{indexSubterms}: Whether or not to index subterms. With this setting switched
off terms are only indexed at the top level. This is very useful as subterm indexing
is very slow and only required for superposition.
\item \textbf{indexPureBG}: Whether or not to index pure background terms. Useful
since this is not required for superposition.
\item \textbf{eqnToTerm}: Whether or not to convert equations to terms. In Section
\ref{sec:simpprob} we pointed out that Negative Unit Simplification
converts equations to terms joined by $\$equal$. Thus our Fingerprint Index
must be able to index these converted terms.
\item \textbf{comparator}: The comparison function used to compare Fingerprint Features.
This function must implement a comparison table such as those seen in Section 
\ref{sec:fingerprints} or Figure \ref{tab:extunif}. Passing a different
function here allows creation of separate indices for matching and unification.
\end{itemize}


\subsection{Applying new Indices to Simplification}


\section{Tailoring to \Beagle's \HSWAC}
\label{sec:tailored}

In this section we discuss the thought process in developing and implementing
extensions to Fingerprint Indexing in order to better tailor it to \beagle's
rather unique logical calculus.

\subsection{Extending the Unification Table with Term Layers}

In the {\HSWAC} all terms have 
a concept of being 'Foreground' or 'Background'. In Section \ref{sec:beagle} we
discussed this concept; referring to it as the \emph{layer} of a term. It is
worth noting at this stage that computing the layer of a term is cheap (or rather,
zero, as it is computed on the fly during term generation and stored for later use).

Recall the four original fingerprint feature symbols from Section \ref{sec:indexing}:
\begin{itemize}
\item $f$: arbitrary constant function symbols.
\item \textbf{A}: Variable at the exact position.
\item \textbf{B}: A variable could be expanded to meet the position.
\item \textbf{N}: Position can never exist regardless of variable assignment.
\end{itemize}
We introduce two new fingerprint features: \textbf{A}+ and \textbf{B}+.
These symbols will be used for the same purpose as the original \textbf{A} and \textbf{B}, but
only for \emph{background} or \emph{abstraction} sorted variables. These variables
can only be used for pure background terms; a fact we may use to restrict the possible
matches for unification.

The layered-ness of function symbols is also relevant to our comparison.
$f$+ in the following table signifies a position where the entire subterm from this position downwards
is `pure background'. Keep in mind that this definition is slightly different
to the definition for \textbf{A}+ and \textbf{B}+; as we must consider all function
symbols below $f$ itself.

At this point it is important to note that these added fingerprint features slightly modify
the original \textbf{A}, \textbf{B} and $f$ features. These features will now
only represent the foreground layered positions.

Table \ref{tab:extunif} displays the unification table with our new background
feature symbols. The table has grown to be a considerable size.
Refer to the original unification table (Table \ref{tab:unif}) for an in-depth
explanation of how this table should be interpreted \cite{shulz12}.

\begin{table}[h]\begin{center}
  \caption{Fingerprint matches for unification; extended by considering term layers.}
  \label{tab:extunif}
  \begin{tabular}{| c || c | c | c | c | c || c | c | c | c |}
  \hline
            &  $f_1$  &  $f_2$  &  \textbf{A} &  \textbf{B} &  \textbf{N} &    $f_1$+  & $f_2$+  & \textbf{A}+ & \textbf{B}+ \\ \hline \hline
  $f_1$     &  \compY &  \compN &  \compY     &  \compY     &  \compN     &    \compN  & \compN  & \compN      & \compN      \\ 
  $f_2$     &  \compN &  \compY &  \compY     &  \compY     &  \compN     &    \compN  & \compN  & \compN      & \compN      \\ 
\textbf{A}  &  \compY &  \compY &  \compY     &  \compY     &  \compN     &    \compY  & \compY  & \compY      & \compY      \\
\textbf{B}  &  \compY &  \compY &  \compY     &  \compY     &  \compY     &    \compY  & \compY  & \compY      & \compY      \\ 
\textbf{N}  &  \compN &  \compN &  \compN     &  \compY     &  \compY     &    \compN  & \compN  & \compN      & \compY      \\ \hline \hline
%
$f_1$+      &  \compN &  \compN &  \compY     &  \compY     &  \compN     &    \compY  & \compN  & \compY      & \compY      \\ 
$f_2$+      &  \compN &  \compN &  \compY     &  \compY     &  \compN     &    \compN  & \compY  & \compY      & \compY      \\ 
\textbf{A}+ &  \compN &  \compN &  \compY     &  \compY     &  \compN     &    \compY  & \compY  & \compY      & \compY      \\
\textbf{B}+ &  \compN &  \compN &  \compY     &  \compY     &  \compY     &    \compY  & \compY  & \compY      & \compY      \\ \hline
  \end{tabular}
\end{center}\end{table}

Note that as this table is for unification it is symmetric along the leading diagonal (as in
the original unification table); so we need only discuss the lower triangle of the matrix.
Furthermore, notice that the bottom right segment of the table is actually identical to
the original unification table. This is expected as when we compare two
pure background features the comparison behaves normally.

We will justify the new section of the table line by line:
\begin{itemize}
\item Background function symbols ($f$+): Recall that this feature is only applicable
if the entire subterm below $f$ is pure background. Therefore it does not
match the foreground version of the same symbol. It does however match both
\textbf{A} and \textbf{B}. This is required since these symbols still match `\emph{impure}' background variables;
which may be expanded to either foreground or pure background terms.
\item Abstraction variables (\textbf{A}+): Similarly to the pure background function symbol
feature, this feature cannot match any terms which sit in the foreground. It can however
match both \textbf{A} and \textbf{B} as they may represent either foreground or background
expressions.
\item Potential expansion of an abstraction variable (\textbf{B}+): Same as for \textbf{A}+
but can also match \textbf{N}.
\end{itemize}

To go with this table we present its corresponding Scala matching code in Listing \ref{lst:extuni}.
Unfortunately the steep increase in table size results in the amount of code required exploding.
It also becomes impossible to use our earlier trick of Set matching (from Listing \ref{lst:unitable}); due to the need for parameterised
Fingerprint symbols (i.e. \textbf{A}+ and \textbf{B}+ represented as FPA(true) and FPB(true) ).
\begin{listing}[H]
\begin{scalacode}
 /** Check two Fingerprint features for compatibility based
   * on the *extended* unification table (See table in report).*/
  def compareFeaturesForUnification
      (a:FPFeature, b:FPFeature) : Boolean = 
  (a,b) match {
    case (FPF(f1), FPF(f2))    => (f1.op == f2.op) && 
                                  (if (f1.isFG || f2.isFG) 
                                      (!f1.isPureBG && !f2.isPureBG)
                                   else true)
    case (FPF(f), FPB(true)) => f.isPureBG
    case (FPB(true), FPF(f)) => f.isPureBG
    case (_, FPB(_))         => true
    case (FPB(_), _)         => true
    case (FPF(f), FPA(true)) => f.isPureBG
    case (FPA(true), FPF(f)) => f.isPureBG
    case (FPN, FPA(_))       => false
    case (FPA(_), FPN)       => false
    case (_, FPA(_))         => true
    case (FPA(_), _)         => true
    case (FPN, FPN)          => true
    case _                   => false
  }
\end{scalacode}
\caption{Scala code to extract fingerprint features for extended layer matching.}
\label{lst:extuni}
\end{listing}

\subsection{Extended Matching Table}

\subsection{Other Tailored Optimisations}

The inference rules in the \HSWAC\ carry with them \emph{far} more restrictions
than the standard superposition calculus. There are several ways which we can
use these restrictions to further optimise term indexing.

\subsubsection{Pure Background Terms}
Pure Background Terms may never be a part of any superposition inference. We can
safely exclude these from our normal Term Index so long as we allow them to
be included for simplification indices.

\subsubsection{Maximal Literals}
The superposition inference rules also require that the two Literals used are
\emph{strictly maximal} in their respective Clauses. \Beagle\ stores these
\emph{inference eligible} maximal Literals in a list. By only indexing these
Literals we save a great deal of space and time. Note that we \emph{do not} need
to bypass this optimisation for the simplification indices; as they only index
unit clauses.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
