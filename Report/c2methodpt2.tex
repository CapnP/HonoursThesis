\section{Adding Indexing to \Beagle}
Our Fingerprint Index class is now fully capable of indexing terms for inference
with the \HSWAC. Our task now is to add the indexer itself into \beagle's inference
loop and make use of it wherever appropriate.

Recall our analysis of \beagle's main loop from Section \ref{sec:mloop}; where we identified
the two sections most appropriate for being augmented with term indexing. We will
start by adding indexing to the \emph{superposition} inference rules; which is
the primary way \beagle\ and the \HSWAC\ creates new information.

\subsection{Attaching a Fingerprint Index}

Actually making use of our Fingerprint Index class will require significant modification
to \beagle's structure and proving sequence. In particular we will need
to add an Index object and replace any occurrences of searching for unification matches
to include indexing.

Originally indexing was included by adding a single Fingerprint Index to the main
class of \beagle. This index was initialised with all Clauses in the input knowledge set,
and was given new Clauses whenever they were created. This setup caused several
problems:
\begin{itemize}
\item \textbf{Redundant Clauses:} Some of \Beagle's operations (in particular Simplification, see Section \ref{sec:simprules})
can cause Clauses to become \emph{redundant}, and no longer required for inference.
These Clauses would remain in the Fingerprint Index; causing clutter and unnecessary
computation.
\item \textbf{Difficult to Split:} The Split rule (see Section \ref{sec:mloop})
could no longer be used since the Fingerprint Index could not easily be reproduced or
duplicated.
\item \textbf{No `Age' Differentiation:} Recall that \beagle\ maintains
two collections of Clauses, \verb!old! and \verb!new! (see Section \ref{sec:mloop}).
With only one Index we currently have no way of identifying which ClauseSet an indexed
Term has come from. As a result, superposition becomes unnecessarily cluttered;
as it only needs to be run against Clauses from \verb!old!.
\end{itemize}
These issues could potentially have been resolved with more careful management of
the Terms in our Index, but a more elegant and simple solution exists.


Clauses from \verb!new! are removed one by one and possibly made redundant before
moving to \verb!old!. \verb!old! itself however is more static; it only ever
has clauses \emph{added} to it. 
Considering this we will add an Index to the ClauseSet class, rather than attach a single Fingerprint Index to the entire \beagle\ inference
process. This makes it possible to individually index each ClauseSet.
By only indexing the \verb!old! set we ensure
no redundant clauses appear in inferences; and it becomes easy to only use
Clauses from \verb!old! for superposition.

Some thought is still required on how to fix the functionality of the Split rule.
Split must be able to copy \beagle's current state to create a new parallel instance;
so it must be able to copy a Fingerprint Index. When Split is activated it calls
the \verb!clone! method of the two main ClauseSets. We will have this method also copy
the associated Index. We can copy an Index either by re-adding all terms to a 
new Fingerprint Index or by creating a deep copy of the Index pointer structure.

\subsection{Indexing Superposition}
Now that our ClauseSets are actively being Indexed we must start to make
use of these Indices for superposition. As stated above
superposition will only require the use of the \verb!old! Clause Index.
We are now capable of performing this retrieval; but we must still 
significantly modify \beagle's existing superposition code before we may make use
of the retrieved terms.

Recall the two superposition inference rules in the \HSWAC\ (taken unmodified
from \cite{baum13}):
\begin{align*}
\textbf{Positive Superposition} &&& \frac{l \approx r \lor C\quad \quad s[u] \approx t \lor D}{\text{abstr}((s[r] \approx t \lor C \lor D)\sigma)} 
\intertext{\tcent{Where
(i) $\sigma = $ simple mgu $(l,u)$,
(ii) $u$ is not a variable,
(iii) $r\sigma \not\succeq l\sigma$,
(iv) $t\sigma \not\succeq s\sigma$,\\
(v) $l$ and $u$ are not pure background terms,
(vi) $(l \approx r)\sigma$ is strictly maximal in $(l \approx r \lor C)\sigma$, and
(vii) $(s \approx t)\sigma$ is strictly maximal in $(s \approx t \lor D)\sigma$ }}
\textbf{Negative Superposition} &&& \frac{l \approx r \lor C\quad \quad s[u] \not\approx t \lor D}{\text{abstr}((s[r] \not\approx t \lor C \lor D)\sigma)}
\intertext{\tcent{Where 
(i) $\sigma = $ simple mgu $(l,u)$,
(ii) $u$ is not a variable,
(iii) $r\sigma \not\succeq l\sigma$,
(iv) $t\sigma \not\succeq s\sigma$,\\
(v) $l$ and $u$ are not pure background terms,
(vi) $(l \approx r)\sigma$ is strictly maximal in $(l \approx r \lor C)\sigma$, and
(vii) $(s \not\approx t)\sigma$ is strictly maximal in $(s \not\approx t \lor D)\sigma$ }}
\end{align*}

When \beagle\ runs these two rules it checks all Clauses in \verb!old! against
a single query Clause selected from \verb!new!. The query clause is tested
for being both the \emph{from} clause ($l \approx r \lor C$) or the \emph{into}
clause ($s[u] \approx t \lor D$). Note that \beagle\ does not split the superposition rules into
two distinct cases; but rather generates all possible negative and positive inferences simultaneously.

Our Fingerprint Index is built to locate Terms likely to \emph{unify}; so
it is condition (i) that our Indexer is most concerned with. Condition (i) states that
$l$ and $u$ must be unifiable by some simple most general unifier $\sigma$ (refer
to Sections \ref{sec:terminology} and \ref{sec:calc} for detailed definitions of
these terms). This condition implies that there are actually two distinct
cases we must cover for indexing; one where $l$ is the query term and one where $u$ is.
These cases correspond to the \emph{from} and \emph{into} cases mentioned
above.

\subsubsection{From Case}
In this case we have a query Clause $l \approx r \lor C$ and wish to find all
subterms $u$ which are likely to unify with the top level term $l$. Note that we must actually
attempt this for all possible $l$s in the Clause, so we must first loop over
each \emph{eligible} Literal. The eligible Literals are those which are positive and capable
of fulfilling condition (vi) in the rules above. If an eligible Literal is unordered we
must also swapping the roles of $l$ and $r$. Note that Clauses maintain a list of their
eligible Literals; and we do not need to search for them.

Once we have found each usable $l$ we use our Term Index to retrieve all terms compatible for unification
(see Section \ref{sec:retrieve}). Since Fingerprint Indexing is \emph{non-perfect} (See Section \ref{sec:fingerprint})
we must make use of \beagle's existing superposition code
to confirm unification; and check the other inference rule conditions. Once we
are sure that the superposition rule applies we may have the existing code
generate the resulting Clause.

\subsubsection{Into Case}
In this case we have a query Clause $s[u] \approx t \lor D$ and wish to find all
top level terms $l$ which are likely to unify with with a subterm $u$. As in the
from case we must find all possible terms which we can use for $s$ while satisfying
maximality and ordering conditions. After selecting a term for $s$ we must 
also then loop over all its subterms, as any of these are a potential choice
for $u$.

So for each subterm $u$ of $s$ we retrieve compatible Terms from the index; but
unlike the from case we are not done here. $l$ is only permitted to be a top level term
so we must actually discard any subterms retrieved from the Index. This may sound expensive in terms
of wasted computation; but filtering in this fashion is cheap, and avoiding this
problem would require an entirely separate Index which indexes only top-level terms.
 
With the potential compatible values for $l$ now retrieved \beagle's existing code
can be used to complete the inference. Our two directional cases are now
covered and superposition is now using our Index to its full potential.

\section{Extending the Fingerprint Index for Simplification}
\label{sec:simp}

\Beagle\ is now fully capable of indexing its terms for superposition. At this stage
we could look into how the Fingerprint Index could be improved; but it is likely
to be far more effective to re-examine where \beagle\ now spends most of its runtime
and investigate other inference procedures for which our Indexer could be applied to. 

We again refer to the analysis of \beagle's inference procedure from Section \ref{sec:mloop}.
In this Section we noted that there are only two subroutines of \beagle\ that require
searching through the ClauseSets; one being superposition and the other being simplification.
So simplification is the only other area where our Indexer may be applied to
any significant effect, and doing so is likely to provide a significant performance
increase.

To confirm how effective Indexing simplification could be we may refer to the results when
instrumenting \beagle\ in VisualVM. The results in Section \ref{section:instr}
indicate (as expected) that simplification often takes up a significant portion
of \beagle's runtime.

We will now attempt to apply Fingerprint Indexing to simplification; beginning
by investigating the current implementation of \beagle's simplification rules. 

\subsection{\Beagle's Simplification Process}
\label{sec:simprules}
\Beagle\ has several simplification rules to aid the logical inference process.
These rules are not technically part of the actual rule based calculus
(hence they are not mentioned in Section \ref{sec:calc}) but rather
implement some special cases of those rules. Providing separate implementations
of these cases can provide a significant speed-up in problems where they occur
frequently.

\Beagle's Clause simplification process (seen in Listing \ref{lst:simpl}) does
three things. First, it \emph{demodulates} the Clause (Section \ref{sec:demod}).
Second, it performs \emph{Negative Unit Simplfication} on the newly demodulated
Clause (Section \ref{sec:negunit}). Finally, it checks if the resulting Clause 
is \emph{subsumed} by an existing one (see Section \ref{sec:terminology}).

\begin{listing}[H]
\begin{lstlisting}
Simplify(select,new,old):
  posUnits := Filter (old U new) for positive, ordered unit Clauses 
  negUnits := Filter (old U new) for negative unit Clauses 
  simpl := Demodulate(select, posUnits)
  simpl := NegativeUnitSimplification(simpl, negUnits)
  if a Clause in (old U new) subsumes simpl
    return Empty Clause
  else
    return simpl 
\end{lstlisting}
\caption{Pseudocode for \beagle's Clause simplification procedure.}
\label{lst:simpl}
\end{listing}



\subsubsection{Negative Unit Simplification}
\label{sec:negunit}

Here we outline the process of Negative Unit Simplification, mentioned in
Listing \ref{lst:simpl} above. It is used to remove Literals from Clauses
which we know trivially cannot be true. Stated more precisely as an inference
rule we have:

\begin{align*}
\textbf{Negative Unit Simplification} &&& \frac{l\not\approx r \quad \quad s \approx t  \lor C}{C}
\intertext{\tcent{Where there exists some \emph{matcher} $\sigma$ such that $(l \approx r)\sigma \equiv s \approx t$\\ (i.e. $l \approx r$ \emph{subsumes} $s \approx t$).
The clause $s \approx t  \lor C$ may be removed.}}
\end{align*}\vspace{-1.3cm}

This essentially says that if we know $l$ is not equal to $r$, then any Literal
stating otherwise may be removed. If we only looked for Literals exactly equal
to $l \approx r$ however the rule would barely ever be applied and not be very useful.
Thus we have Negative Unit Simplification consider any \emph{instances} of $l \approx r$.
A Literal formed by substituting the variables in $l \approx r$ also contradicts
$l \not\approx r$; this is the $l \approx r$ \emph{subsumes} $s \approx t$ condition.
Checking for these subsumed literals is potentially expensive; and it is what
we will attempt to improve with Fingerprint Indexing.

Note that we do not have a concept of \emph{Positive} Unit Simplification 
since it would be covered as a special case of the Demodulation rule.

\subsubsection{Demodulation}
\label{sec:demod}

The demodulation rule was first proposed for use in the superposition calculus by \citeN{demod}.
It allows the removal of variables and subterms which are \emph{redundant}; that is, their
removal will not effect the truth value of any Terms they are removed from. The
Demodulation rules used by \beagle\ follow
(recall that $l \to r$ refers to an ordered Literal $l \approx r$ and $s[u]$ refers
to a Term $s$ with a subterm $u$).

\begin{align*}
\textbf{Demodulation} &&& \frac{l \to r \quad \quad s[u] \approx t  \lor D}{ s[r\sigma] \approx t \lor D}
\intertext{\tcent{Where
$\sigma$ is a matcher from $l$ to $u$. (i.e. $l$ \emph{subsumes} $u$)\\
The clause $s[u] \approx t  \lor D$ may be removed.}}
\textbf{Negative Demodulation} &&& \frac{l \to r \quad \quad s[u] \not\approx t  \lor D}{ s[r\sigma] \not\approx t \lor D}
\intertext{\tcent{Where
$\sigma$ is a matcher from $l$ to $u$. (i.e. $l$ \emph{subsumes} $u$)\\
The clause $s[u] \not\approx t  \lor D$ may be removed.}}
\end{align*}

At the simplest level, Demodulation allows us to replace all occurrences of $l$ by $r$; provided that
we have some unit Clause $l \to r$. The rules above are more general however,
and allow the replacement of any subterm $u$ which is \emph{subsumed} by $l$.
As in Negative Unit Simplification, finding terms which pass this subsumption check is most time consuming.
Furthermore, in Demodulation we must perform this search for 
all \emph{subterms} of $s$; making Demodulation a prime candidate for Indexing.

\subsection{Generalising our Fingerprint Index}
\label{sec:config}

Attempting to directly apply the existing Fingerprint Index to Negative Unit Simplification and
Demodulation proved difficult and produced a number of significant problems:
\begin{itemize}
\item \textbf{Term Matching:} The above simplification rules all try to find Terms which
\emph{match} rather than those that \emph{unify}. Our Index so far has only
been built to support unification; as that is what is used by superposition.
\item \textbf{Cluttered Index:} In both Negative Unit Simplification and Demodulation we are simplifying
a single Clause against a list of unit Clauses. These top-level unit Clauses are
all we wish to retrieve from the Index, but our current implementation would retrieve
any matching Term objects and even any matching subterms; resulting in many
of the retrieved matches being thrown away.
\item \textbf{Indexing Equations:} Up to this point we have only ever been interested
in using a Term as the query object for our Index. Notice however that in Negative
Unit Simplification we are trying to find matches for an \emph{Equation}. In other areas of \beagle\ 
this issue is handled by converting the Equation into a Term using a reserved
function $\$equal$:
\begin{align*}
l \to r &\quad\equiv\quad \$equal (l, r) \\
l \approx r &\quad\equiv\quad \$equal (l, r), \quad  \$equal (r, l)
\end{align*}
After conversion the $\$equal (l, r)$ Term could be used as a query for our index;
but since the current implementation does not convert Equations in this manner
it would be guaranteed to find zero matches.
\end{itemize}

So, obviously using our current Index is not an option; and we must come up with
a different method for indexing simplification. Thus, to free ourselves from
the restrictions of our superposition index, we will create new Fingerprint
Indices built specifically for simplification. Obviously there is no need
to completely redo the existing Fingerprint Indexing code; we need only
make the current implementation more flexible and configurable.

In Listing \ref{lst:config} we introduce an options object to pass to our Fingerprint Index class.
This object will allow us to create multiple Term Indices that behave in different
ways.

\begin{listing}[H]
\begin{scalacode}
/** Configuration object for a Fingerprint Index */
class IndexConfig(
  val positionsToSample : PositionList,
  val indexSubterms     : Boolean,
  val eqnToTerm         : Boolean,
  val comparator        : (FPFeature, FPFeature) => Boolean)
\end{scalacode}
\caption{Class to pass settings to an arbitrary Fingerprint Index. Note that
this class does not require an implementation.}
\label{lst:config}
\end{listing}

The options and their uses are outlined here:

\begin{itemize}
\item \textbf{positionsToSample}: A list of positions indicating what should be sampled
to create term Fingerprints, with the \verb!extractFeature! function from Listing \ref{lst:posextract}.
\item \textbf{indexSubterms}: Whether or not to index subterms. With this setting switched
off terms are only indexed at the top level. This allows us to clear up unnecessary clutter
for indexing simplification.
\item \textbf{eqnToTerm}: Whether or not to convert equations to terms. In the list
of problems above we pointed out that Negative Unit Simplification
must convert Equations to Terms joined by $\$equal$. Thus we will require an Index
which stores Equations converted in this manner.
\item \textbf{comparator}: The comparison function used to compare Fingerprint Features.
This function must implement a comparison table such as those seen in Section 
\ref{sec:fingerprints} or Table \ref{tab:extunif}. Passing a different
function here allows the creation of separate Indices for matching and unification.
\end{itemize}

With this options object in place we may easily create three separate
Fingerprint Indices; one for Superposition, one for Negative Unit Simplification and
one for Demodulation. The Simplification Indices will only ever have unit Clauses
added to them meaning that we have far less to search through when trying to perform
simplification. The configuration for our Indices is as follows:
\begin{itemize}
\item \textbf{Superposition Index:} This Index has not changed from the original
implemenation. It has subterm indexing on, Equation conversion off and uses
unification for its comparison function (see Listing \ref{lst:unitable}).
Superposition requires this Index to add any Clauses added to \verb!old!
(see Section \ref{sec:mloop}).
\item \textbf{Negative Unit Index:} Used for indexing only the negative
unit Clauses. It has subterm indexing off, Equation conversion on and uses
matching for its comparison function. Negative Unit Simplification requires
that this Index contains any negative unit Clauses in both \verb!new! and \verb!old!
(see Listing \ref{lst:simpl}).
\item \textbf{Positive Unit Index:} Used for indexing only the positive
unit Clauses. It has subterm indexing off, Equation conversion off and uses
matching for its comparison function. Demodulation requires
that this Index contains any positive, ordered unit Clauses in both \verb!new! and \verb!old!
(see Listing \ref{lst:simpl}).
\end{itemize}

This use of multiple indices introduces a memory overhead. However, as we have mentioned
before, this overhead is negligible since we are generally only concerned
with speed.

\subsection{Applying new Indices to Simplification}

Making use of these tailored Indices for simplification is now fairly trivial;
as \beagle's existing implementation does most of the work. After replacing
loops over unit Clauses with loops over compatible matches from the relevant Index, the
only task remaining is to ensure that the relevant unit Clauses are Indexed.

We will have the unit Clause Indices add Clauses as they are created and added
to \verb!new!. As Clauses are pulled from \verb!new! to be added into \verb!old!
this method would seem to always contain the unit Clauses from both sets.
However, notice in Listing \ref{lst:main} that Clauses are simplified after being
selected out of \verb!new!. This means that if simplification generates a unit
Clause it will not be added into the Indices. So we must index Clauses as they
added to \verb!new! \emph{and} as they are added to \verb!old!.

Unfortunately this leads to many redundant Clauses in our Indices. If a unit Clause
is selected from \verb!new! there is a possibility it will simplify to the empty
Clause and it will no longer be needed for inference. Otherwise it will be added
into \verb!old! and appear in the relevant index twice. These redundant and 
duplicate clauses could be resolved by adding the ability to \emph{remove} from
an Index. This would take significant effort however; and the extra Clauses are
unlikely to cause any problems apart from a few unnecessary inferences and a negligible slowdown. 

\section{Tailoring to \Beagle's \HSWAC}
\label{sec:tailored}

\Beagle's most time consuming search procedures now all make use of one or more
Fingerprint Indices. With this fully implemented we may turn our attention
toward improving lookup in the Index itself.
In this section we discuss the development of some original improvements for Fingerprint Indexing,
designed to specifically tailor it to \beagle's
rather unique \HSWAC.

\subsection{The Extended Hierarchical Unification Table}

In the {\HSWAC} all terms have 
a concept of being 'Foreground' or 'Background' (see Section \ref{sec:beagle});
this is what makes the calculus hierarchical. 
Here we will attempt to use this distinction to our advantage in Fingerprint Indexing
by only retrieving Terms which exist in the same hierarchy position. It is
worth noting at this stage that computing where a term exists in the hierarchy is cheap (or rather,
zero, as it is computed on the fly during term generation and stored for later use).

Recall the four original fingerprint feature symbols from Section \ref{sec:indexing}.
These features are computed by sampling a Term object at some position; and can be compared
with a table to check if two Terms may unify or match.
\begin{itemize}
\item $f$: arbitrary constant function symbols.
\item \textbf{A}: Variable at the exact position.
\item \textbf{B}: A variable could be expanded to meet the position.
\item \textbf{N}: Position can never exist regardless of variable assignment.
\end{itemize}
We introduce two new fingerprint features: \textbf{A}+ and \textbf{B}+.
These symbols will be used for the same purpose as the original \textbf{A} and \textbf{B}, but
only for \emph{pure background} or \emph{abstraction} variables. These variables
can only be used for pure background terms; a fact we may use to restrict the possible
matches for unification.

We also wish to ensure that function symbols match in terms of being foreground or
background. Since we require function symbols to be equal (see the unification and
matching tables, Table \ref{tab:unif} and \ref{tab:match}) we would expect this
to occur automatically; but this is not necessarily the case. A background function symbol
may have foreground terms as arguments; in which case it will not unify with a
pure background term. Thus we introduce another new Fingerprint Feature for background function
symbols, $f$+. This symbol signifies a position where the entire subterm downwards from $f$
is pure background. Keep in mind that this definition is slightly different
to the definition for \textbf{A}+ and \textbf{B}+ where we only consider the variable
at the precise position.

Note that by introducing these symbols we have slightly modified the definition
of the original \textbf{A}, \textbf{B} and $f$ features. These features will now
only represent terms not covered by the + symbols, that is, positions with foreground or impure
background terms.

Table \ref{tab:extunif} displays the unification table with our new background
feature symbols. By introducing four new symbols we have grown the table to be a considerable size.
Refer to the original unification table (Table \ref{tab:unif}) for an in-depth
explanation of how this table should be interpreted \cite{shulz12}.

\begin{table}[H]\begin{center}
  \caption{Fingerprint comparison table for unification; extended by considering the term hierarchy.}
  \label{tab:extunif}
  \begin{tabular}{| c || c | c | c | c | c || c | c | c | c |}
  \hline
            &  $f_1$  &  $f_2$  &  \textbf{A} &  \textbf{B} &  \textbf{N} &    $f_1$+  & $f_2$+  & \textbf{A}+ & \textbf{B}+ \\ \hline \hline
  $f_1$     &  \compY &  \compN &  \compY     &  \compY     &  \compN     &    \compN  & \compN  & \compN      & \compN      \\ 
  $f_2$     &  \compN &  \compY &  \compY     &  \compY     &  \compN     &    \compN  & \compN  & \compN      & \compN      \\ 
\textbf{A}  &  \compY &  \compY &  \compY     &  \compY     &  \compN     &    \compY  & \compY  & \compY      & \compY      \\
\textbf{B}  &  \compY &  \compY &  \compY     &  \compY     &  \compY     &    \compY  & \compY  & \compY      & \compY      \\ 
\textbf{N}  &  \compN &  \compN &  \compN     &  \compY     &  \compY     &    \compN  & \compN  & \compN      & \compY      \\ \hline \hline
%
$f_1$+      &  \compN &  \compN &  \compY     &  \compY     &  \compN     &    \compY  & \compN  & \compY      & \compY      \\ 
$f_2$+      &  \compN &  \compN &  \compY     &  \compY     &  \compN     &    \compN  & \compY  & \compY      & \compY      \\ 
\textbf{A}+ &  \compN &  \compN &  \compY     &  \compY     &  \compN     &    \compY  & \compY  & \compY      & \compY      \\
\textbf{B}+ &  \compN &  \compN &  \compY     &  \compY     &  \compY     &    \compY  & \compY  & \compY      & \compY      \\ \hline
  \end{tabular}
\end{center}\end{table}

Note that as this table is for unification it is symmetric along the leading diagonal (as in
the original unification table); so we need only discuss the lower triangle of the matrix.
Furthermore, notice that the bottom right segment of the table is actually identical to
the original unification table. This is expected as when we compare two
pure background features the comparison behaves normally.

So we need only justify the lower-left quadrant of the table.
We will justify this new section line by line:
\begin{itemize}
\item \textbf{Pure Background Function Symbols} ($f$+): Recall that this feature is only applicable
if the entire subterm below $f$ is pure background. Therefore it does not
match the foreground version of the same symbol. It does however match both
\textbf{A} and \textbf{B}. This is required since these symbols still match \emph{impure} or \emph{ordinary} background variables;
which may be expanded to either foreground or pure background terms.
\item \textbf{Abstraction Variables} (\textbf{A}+): Similarly to the pure background function symbol
feature, this feature cannot match any terms which sit in the foreground. It can however
match both \textbf{A} and \textbf{B} as they may represent either foreground or background
expressions.
\item \textbf{Potential Expansion of an Abstraction Variable} (\textbf{B}+): Same as for \textbf{A}+
but can also match \textbf{N} (as we may choose not to generate the position).
\end{itemize}

To go with this table we present its corresponding Scala code in Listing \ref{lst:extuni}.
To accommodate our new Fingerprint Features we will convert our original \verb!FPA! and \verb!FPB!
(see Section \ref{sec:datatypes}) objects to take a boolean argument.
We then represent \textbf{A}+ and \textbf{B}+ as \verb!FPA(true)! and \verb!FPB(true)! respectively.
Unfortunately the steep increase in table size results in the amount of code required exploding.
This is primarily due to that fact that we can no longer reduce the table to four
simple cases as in Section \ref{sec:retrieve}. In that Listing we also
used \verb!Set! objects so that we did not need to provide a separate check for each
direction. We can no longer use that trick since our Fingerprint Features are now
parameterised instances rather than static objects.
\begin{listing}[H]
\begin{scalacode}
 /** Check two Fingerprint features for compatibility based
   * on the *extended* unification table (See table in report).*/
  def compareFeaturesForUnification
      (a:FPFeature, b:FPFeature) : Boolean = 
  (a,b) match {
    case (FPF(f1), FPF(f2))    => (f1.op == f2.op) && 
                                  (if (f1.isFG || f2.isFG) 
                                      (!f1.isPureBG && !f2.isPureBG)
                                   else true)
    case (FPF(f), FPB(true)) => f.isPureBG
    case (FPB(true), FPF(f)) => f.isPureBG
    case (_, FPB(_))         => true
    case (FPB(_), _)         => true
    case (FPF(f), FPA(true)) => f.isPureBG
    case (FPA(true), FPF(f)) => f.isPureBG
    case (FPN, FPA(_))       => false
    case (FPA(_), FPN)       => false
    case (_, FPA(_))         => true
    case (FPA(_), _)         => true
    case (FPN, FPN)          => true
    case _                   => false
  }
\end{scalacode}
\caption{Scala code implementing the Extended Hierarchical Unification Table.}
\label{lst:extuni}
\end{listing}

%\subsection{Extended Matching Table}

\subsection{Other Tailored Optimisations}

The inference rules in the \HSWAC\ carry with them \emph{far} more restrictions
than the standard superposition calculus. There are several ways which we can
use these restrictions to further optimise term indexing.

\subsubsection{Pure Background Terms}
Pure Background Terms may never be a part of any superposition inference.
This includes not only top-level Terms but also any pure background subterms we encounter.
We can safely exclude any of these Terms from our superposition Term Index so long as we allow them to
be included for simplification indices. To do this we will add another flag
to the Fingerprint Index configuration object (see Listing \ref{lst:config}),
\verb!IndexPureBG!, which controls the indexing of pure background Terms.

\subsubsection{Maximal Literals}
The superposition inference rules also require that the two Literals used are
\emph{strictly maximal} in their respective Clauses. \Beagle\ stores these
\emph{inference eligible} maximal Literals in a list. By only indexing these
Literals we save a great deal of space and time. Pure background Literals
are also considered ineligible. This suits our needs for superposition but means
that we must index ineligible Literals for simplification. As with the pure background Terms
optimisation we implement a new configuration flag \verb!IndexIneligible!.
